# -*- coding: utf-8 -*-
"""MLG_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19mhyl73HQORXyDQFVqMmYfFMhW6uzXjs
"""

#importing data

import pandas as pd
import numpy as np
import seaborn as sns

raw_data = pd.read_csv('tmdb_5000_movies.csv')

cast_data = pd.read_csv('tmdb_5000_credits.csv')

cast_data.keys()

joined_data = raw_data.merge(cast_data,left_on='id', right_on='movie_id', how='inner')

joined_data.keys()

joined_data.head(3)

raw_data.head(3)

joined_data.keys()

data=joined_data[['title_x','budget','revenue','vote_average','vote_count','runtime','genres','spoken_languages','production_countries','release_date','cast']].copy()

data.keys()

data.head(1)

data.tail(1)

data.isnull().sum()

data.dropna(inplace=True)

data.head(1)

data.keys()

data = data[data['revenue']>=100000]

"""done taking data"""

data.info()

data.tail(1)

data['genres']

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in data.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

import ast

df = pd.DataFrame(data)

def extract_genre_names(genres_str):
    genres_list = ast.literal_eval(genres_str)
    return [genre['name'] for genre in genres_list]

df['genres'] = df['genres'].apply(extract_genre_names)

df_exploded = df.explode('genres')

df_one_hot = pd.get_dummies(df_exploded, columns=['genres'], prefix='', prefix_sep='')

df_one_hot = df_one_hot.groupby('title_x').max().reset_index()

df_one_hot.set_index('title_x', inplace=True)

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in df_one_hot.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

df = pd.DataFrame(df_one_hot)

def extract_names(json_str):
    items_list = ast.literal_eval(json_str)
    return [item['name'] for item in items_list]

df['spoken_languages'] = df['spoken_languages'].apply(extract_names)

df_exploded_languages = df.explode('spoken_languages')

df_one_hot_languages = pd.get_dummies(df_exploded_languages, columns=['spoken_languages'], prefix='', prefix_sep='')

df_one_hot_languages = df_one_hot_languages.groupby('title_x').max().reset_index()

df_one_hot_languages.set_index('title_x',inplace=True)

df_one_hot_languages.head(1)

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in df_one_hot_languages.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

df = pd.DataFrame(df_exploded_languages)

def extract_country_names(country_str):
    country_list = ast.literal_eval(country_str)
    return [country['name'] for country in country_list]

df['production_countries'] = df['production_countries'].apply(extract_country_names)
df_exploded = df.explode('production_countries')
df_one_hot = pd.get_dummies(df_exploded, columns=['production_countries'], prefix='', prefix_sep='')
df_one_hot = df_one_hot.groupby('title_x').max().reset_index()
df_one_hot.set_index('title_x', inplace=True)

df_one_hot.shape

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in df_one_hot.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

df_combined=df_one_hot.copy()

df_combined['cast']

df = pd.DataFrame(df_combined)

def count_actor_occurrences(data):
    actor_counts = {}
    for row in data:
        cast_list = ast.literal_eval(row)
        for actor in cast_list:
            actor_name = actor['name']
            actor_counts[actor_name] = actor_counts.get(actor_name, 0) + 1
    return actor_counts

actor_counts = count_actor_occurrences(df['cast'])

def calculate_star_power(row):
    cast_list = ast.literal_eval(row)
    leading_cast = cast_list[:2]  # Consider only the first two cast members
    star_power = sum(actor_counts.get(actor['name'], 0) for actor in leading_cast)
    return star_power

df['star_power'] = df['cast'].apply(calculate_star_power)

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in df_combined.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

df.drop(columns=['cast'], inplace=True)

df.head(1)

df.drop('spoken_languages', axis=1, inplace=True)

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in df.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

df.shape

data=df.copy()

df = pd.DataFrame(data)

df['release_date'] = pd.to_datetime(df['release_date'])
df['Day of Week'] = df['release_date'].dt.day_name()
days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
for day in days_of_week:
    df[day] = (df['Day of Week'] == day).astype(int)
df.drop(columns=['release_date', 'Day of Week'], inplace=True)

pd.set_option('display.max_columns', None)

df.keys()

for column in df.columns:
    non_numeric_values = df[column].apply(lambda x: isinstance(x, str) and not x.isdigit())
    if non_numeric_values.any():
        print(f"Column '{column}' contains non-numeric values:")
        print(df[column][non_numeric_values])

df.shape

final_data=df.copy()

from sklearn.linear_model import LinearRegression

y=final_data['revenue'].values

final_data_x=final_data.drop('revenue',axis=1)

final_data_x.head(1)

x=final_data_x.values

x

y

from sklearn.model_selection import train_test_split

X_train , X_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1)

X_train.shape

linear_model = LinearRegression()
linear_model.fit(X_train,y_train)

y_pred = linear_model.predict(X_test)

coef=linear_model.coef_

coef

df = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score

print("MAE",mean_absolute_error(y_test,y_pred))
print("MSE",mean_squared_error(y_test,y_pred))
print("RMSE",np.sqrt(mean_squared_error(y_test,y_pred)))
print("R2_Score",r2_score(y_test,y_pred))

import matplotlib.pyplot as plt

from sklearn.model_selection import cross_val_score

scores= cross_val_score(linear_model,x,y,cv=10,scoring='r2')

scores

train_score=linear_model.score(x,y)

train_score

column_names_array = []

# Iterate through each column name and append it to the array
for column_name in final_data.columns:
    column_names_array.append(column_name)

# Print the array of column names
print(column_names_array)

from  sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree = 2, include_bias=True)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model2=LinearRegression()

X_train.shape

X_test_poly.shape

model2.fit(X_train_poly,y_train)

y_pred2=model2.predict(X_test_poly)

print("MAE",mean_absolute_error(y_test,y_pred2))
print("MSE",mean_squared_error(y_test,y_pred2))
print("RMSE",np.sqrt(mean_squared_error(y_test,y_pred2)))
print("R2_Score",r2_score(y_test,y_pred2))

from sklearn.decomposition import PCA

pca = PCA(n_components=50)
X_train_trf = pca.fit_transform(X_train)
X_test_trf = pca.transform(X_test)

X_train_trf.shape

from  sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree = 2, include_bias=True)
X_train_poly = poly.fit_transform(X_train_trf)
X_test_poly = poly.transform(X_test_trf)

model3= LinearRegression()

model3.fit(X_train_poly,y_train)

y_pred3=model3.predict(X_test_poly)

print("MAE",mean_absolute_error(y_test,y_pred3))
print("MSE",mean_squared_error(y_test,y_pred3))
print("RMSE",np.sqrt(mean_squared_error(y_test,y_pred3)))
print("R2_Score",r2_score(y_test,y_pred3))

from sklearn.linear_model import ElasticNet

elastic_net = ElasticNet(alpha=100.0, l1_ratio=0.0)

elastic_net.fit(X_train, y_train)

lassopred = elastic_net.predict(X_test)

print("MAE",mean_absolute_error(y_test,lassopred))
print("MSE",mean_squared_error(y_test,lassopred))
print("RMSE",np.sqrt(mean_squared_error(y_test,lassopred)))
print("R2_Score",r2_score(y_test,lassopred))

from sklearn.tree import DecisionTreeRegressor

tree_model = DecisionTreeRegressor()
tree_model.fit(X_train, y_train)

tree_predictions = tree_model.predict(X_test)

print("MAE",mean_absolute_error(y_test,tree_predictions))
print("MSE",mean_squared_error(y_test,tree_predictions))
print("RMSE",np.sqrt(mean_squared_error(y_test,tree_predictions)))
print("R2_Score",r2_score(y_test,tree_predictions))

xcopy=final_data_x

xcopy.head(1)

columns_to_select = ['budget', 'vote_average', 'vote_count', 'runtime', 'star_power']

xnumer=xcopy[columns_to_select]

xnumer.head(1)

xbool=xcopy.iloc[:,4:]

xbool.head(1)

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

scaler = StandardScaler()
scaled_data = scaler.fit_transform(final_data)

wcss = []  # Within-cluster sum of squares

for k in range(1, 31):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.figure(figsize=(10, 8))
plt.plot(range(1, 31), wcss, marker='o', linestyle='--')
plt.xlabel('Number of clusters (k)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal k')
plt.show()

"""Ensembling"""

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import VotingRegressor

lm2=LinearRegression()

polmod=make_pipeline(PolynomialFeatures(degree=2),LinearRegression)

ensemble=VotingRegressor([('lr',lm2),('poly',polmod)])

boolean_columns = x.select_dtypes(include=['bool']).columns
x[boolean_columns] = x[boolean_columns].astype(int)

ensemble.fit(X_train,y_train)

